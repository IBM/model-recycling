---
layout: default
title: Home
nav_order: 0
image: "Twitter_card.png"
description: "Model-recycling - the best model per architecture. Comparing finetuned models from HF, as base models for future finetuning. "

---
# Welcome to model-recycling page

Hardly anyone trains from scratch anymore, we all finetune over a pretrained model. 

[Research](https://arxiv.org/abs/2211.00107) slowly reaches consensus that some finetuned models are better base models than the pretrained models
themselves.

This site presents a dynamic view of the best models to choose for a given model size and architecture.
We follow the findings and methodology from our [paper](https://arxiv.org/abs/2211.00107):
We download finetuned models found in HuggingFace per architecture and efficiently rank them over a representative task.
We then evaluate the top ranked models by finetuning over a large set of 36 target tasks, and report the average
performance of each base model.

Tested so far: 3684 (and counting)
## Best models per architectures
<br>

| Pretrained                                                                   | Best model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |   Avg. |   Pretrained Avg. | Ranking                                 |
|:-----------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------:|------------------:|:----------------------------------------|
| [roberta-base](roberta-base_pretrain_scores_table)                           | [ibm/ColD-Fusion](model_gain_chart?avg=2.25&mnli_lp=nan&20_newsgroup=0.54&ag_news=0.03&amazon_reviews_multi=-0.32&anli=1.59&boolq=2.68&cb=19.73&cola=-0.22&copa=23.30&dbpedia=1.34&esnli=0.15&financial_phrasebank=2.99&imdb=-0.04&isear=1.06&mnli=0.31&mrpc=-0.86&multirc=2.50&poem_sentiment=1.63&qnli=-0.00&qqp=0.40&rotten_tomatoes=3.41&rte=12.80&sst2=1.30&sst_5bins=-0.30&stsb=1.38&trec_coarse=-0.11&trec_fine=2.64&tweet_ev_emoji=0.00&tweet_ev_emotion=1.22&tweet_ev_hate=1.55&tweet_ev_irony=6.37&tweet_ev_offensive=1.38&tweet_ev_sentiment=-0.60&wic=3.17&wnli=-6.90&wsc=-2.69&yahoo_answers=-0.53&model_name=ibm%2FColD-Fusion&base_name=roberta-base)                                                              |  78.47 |             76.22 | [link](roberta-base_table)              |
| [bert-base-uncased](bert-base-uncased_pretrain_scores_table)                 | [ibm/ColD-Fusion-bert-base-uncased-itr23-seed0](model_gain_chart?avg=3.44&mnli_lp=nan&20_newsgroup=2.07&ag_news=-0.46&amazon_reviews_multi=0.34&anli=2.14&boolq=5.42&cb=12.41&cola=0.15&copa=8.55&dbpedia=0.04&esnli=1.02&financial_phrasebank=15.57&imdb=0.52&isear=0.22&mnli=0.65&mrpc=5.02&multirc=-0.61&poem_sentiment=18.89&qnli=-0.60&qqp=0.29&rotten_tomatoes=4.55&rte=18.00&sst2=2.18&sst_5bins=2.72&stsb=2.71&trec_coarse=1.14&trec_fine=12.67&tweet_ev_emoji=0.28&tweet_ev_emotion=1.16&tweet_ev_hate=2.20&tweet_ev_irony=0.61&tweet_ev_offensive=-0.37&tweet_ev_sentiment=0.82&wic=2.58&wnli=1.55&wsc=0.38&yahoo_answers=-1.02&model_name=ibm%2FColD-Fusion-bert-base-uncased-itr23-seed0&base_name=bert-base-uncased) |  75.64 |             72.20 | [link](bert-base-uncased_table)         |
| [bert-base-cased](bert-base-cased_pretrain_scores_table)                     | [ellabettison/finetuned_orgnames_bert](model_gain_chart?avg=1.83&mnli_lp=nan&20_newsgroup=1.07&ag_news=0.01&amazon_reviews_multi=0.05&anli=0.18&boolq=1.76&cb=6.16&cola=0.99&copa=2.85&dbpedia=0.93&esnli=0.24&financial_phrasebank=15.04&imdb=0.30&isear=1.56&mnli=-0.26&mrpc=2.37&multirc=-0.63&poem_sentiment=12.12&qnli=-0.61&qqp=0.34&rotten_tomatoes=0.73&rte=1.62&sst2=0.71&sst_5bins=-0.23&stsb=1.17&trec_coarse=0.37&trec_fine=5.62&tweet_ev_emoji=0.68&tweet_ev_emotion=1.88&tweet_ev_hate=1.43&tweet_ev_irony=1.26&tweet_ev_offensive=1.22&tweet_ev_sentiment=0.98&wic=-1.61&wnli=4.01&wsc=1.54&yahoo_answers=0.07&model_name=ellabettison%2Ffinetuned_orgnames_bert&base_name=bert-base-cased)                        |  74.26 |             72.43 | [link](bert-base-cased_table)           |
| [t5-base](t5-base_pretrain_scores_table)                                     | [adit94/nlpcharade](model_gain_chart?avg=2.78&mnli_lp=nan&20_newsgroup=-29.01&ag_news=2.38&amazon_reviews_multi=4.40&anli=1.58&boolq=10.84&cb=-8.92&cola=-2.62&copa=39.82&dbpedia=12.81&esnli=0.60&financial_phrasebank=1.31&imdb=-10.84&isear=26.32&mnli=8.64&mrpc=3.06&multirc=12.08&poem_sentiment=-29.04&qnli=-34.05&qqp=1.74&rotten_tomatoes=-36.72&rte=16.64&sst2=-9.88&sst_5bins=18.68&stsb=-5.99&trec_coarse=-30.77&trec_fine=-0.01&tweet_ev_emoji=47.56&tweet_ev_emotion=10.81&tweet_ev_hate=21.50&tweet_ev_irony=10.21&tweet_ev_offensive=-13.09&tweet_ev_sentiment=16.40&wic=4.61&wnli=0.99&wsc=17.17&yahoo_answers=21.01&model_name=adit94%2Fnlpcharade&base_name=t5-base)                                            |  78.23 |             75.45 | [link](t5-base_table)                   |
| [google/t5-v1_1-base](google_t5-v1_1-base_pretrain_scores_table)             | [talhaa/flant5](model_gain_chart?avg=9.03&mnli_lp=nan&20_newsgroup=4.19&ag_news=1.36&amazon_reviews_multi=0.23&anli=14.13&boolq=17.27&cb=23.12&cola=9.97&copa=29.50&dbpedia=6.50&esnli=5.11&financial_phrasebank=18.16&imdb=0.52&isear=1.43&mnli=11.97&mrpc=13.44&multirc=5.70&poem_sentiment=19.42&qnli=3.74&qqp=7.12&rotten_tomatoes=3.64&rte=25.34&sst2=0.09&sst_5bins=4.72&stsb=20.65&trec_coarse=4.15&trec_fine=9.53&tweet_ev_emoji=13.59&tweet_ev_emotion=4.90&tweet_ev_hate=1.07&tweet_ev_irony=7.25&tweet_ev_offensive=2.16&tweet_ev_sentiment=1.88&wic=12.97&wnli=9.44&wsc=7.45&yahoo_answers=3.38&model_name=talhaa%2Fflant5&base_name=google%2Ft5-v1_1-base)                                                           |  77.86 |             68.82 | [link](google_t5-v1_1-base_table)       |
| [microsoft/deberta-v3-base](microsoft_deberta-v3-base_pretrain_scores_table) | [sileod/deberta-v3-base_tasksource-420](model_gain_chart?avg=1.41&mnli_lp=nan&20_newsgroup=0.63&ag_news=0.46&amazon_reviews_multi=-0.40&anli=0.94&boolq=2.55&cb=10.71&cola=0.49&copa=10.60&dbpedia=0.10&esnli=-0.25&financial_phrasebank=1.31&imdb=-0.17&isear=0.63&mnli=0.42&mrpc=-0.23&multirc=1.73&poem_sentiment=0.77&qnli=0.12&qqp=-0.05&rotten_tomatoes=0.67&rte=2.13&sst2=0.01&sst_5bins=-0.02&stsb=1.39&trec_coarse=0.24&trec_fine=0.18&tweet_ev_emoji=0.62&tweet_ev_emotion=0.43&tweet_ev_hate=1.84&tweet_ev_irony=1.43&tweet_ev_offensive=0.17&tweet_ev_sentiment=0.08&wic=-1.78&wnli=3.03&wsc=9.95&yahoo_answers=0.17&model_name=sileod%2Fdeberta-v3-base_tasksource-420&base_name=microsoft%2Fdeberta-v3-base)        |  80.45 |             79.04 | [link](microsoft_deberta-v3-base_table) |

<br>
<br>

To learn more see our [FAQ](faq) or read the paper. See detailed evaluation results on each architecture [here](Rankings).
If you have any feedback or question please [contact us](contact_us).

<span style="font-size:0.8em;">This work was performed in IBM Research by Leshem Choshen, Elad Venezian, Shachar Don-Yehiya, Noam Slonim and Yoav Katz.</span>
