---
layout: default
title: t5-base
parent: Rankings
has_children: true
---
[comment]: # (This page contains a link to a table with the ranking and performance of all ranked t5-base models. In addition, it contains a table with the baseline and the 10 best models. The original ranking was done by finetuning only the classification head of the model (linear probing) over the MNLI dataset.  The best models  by this ranking where ranked by the average accuracy after finetuning over the 36 datasets (except for the stsb dataset, where we used the Spearman correlation instead of accuracy).)

Ranking and performance of all 36 ranked t5-base models ([full table](./results/t5-base_table.csv)).  13 models are fully tested.

Notes:
1. The baseline results can be found [here](t5-base_pretrain_scores_table)
1. While the average improvement is small, many datasets show large gains
<br>


|            | model_name                             | avg     | mnli_lp   | 20_newsgroup   | ag_news   | amazon_reviews_multi   | anli    | boolq   | cb      | cola    | copa    | dbpedia   | esnli   | financial_phrasebank   | imdb    | isear   | mnli    | mrpc    | multirc   | poem_sentiment   | qnli    | qqp     | rotten_tomatoes   | rte     | sst2    | sst_5bins   | stsb    | trec_coarse   | trec_fine   | tweet_ev_emoji   | tweet_ev_emotion   | tweet_ev_hate   | tweet_ev_irony   | tweet_ev_offensive   | tweet_ev_sentiment   | wic     | wnli    | wsc     | yahoo_answers   |
|:-----------|:---------------------------------------|:--------|:----------|:---------------|:----------|:-----------------------|:--------|:--------|:--------|:--------|:--------|:----------|:--------|:-----------------------|:--------|:--------|:--------|:--------|:----------|:-----------------|:--------|:--------|:------------------|:--------|:--------|:------------|:--------|:--------------|:------------|:-----------------|:-------------------|:----------------|:-----------------|:---------------------|:---------------------|:--------|:--------|:--------|:----------------|
| *baseline* | *t5-base*                              | *75.45* | *nan*     | *85.12*        | *89.42*   | *66.54*                | *47.05* | *76.66* | *75.54* | *81.91* | *49.65* | *76.41*   | *89.72* | *85.30*                | *92.33* | *71.28* | *83.80* | *85.66* | *60.28*   | *74.42*          | *90.38* | *88.94* | *88.61*           | *73.68* | *93.84* | *55.55*     | *85.31* | *97.21*       | *92.33*     | *44.88*          | *79.51*            | *52.74*         | *73.74*          | *84.03*              | *70.21*              | *67.19* | *55.35* | *60.00* | *71.59*         |
| 1          | adit94/nlpcharade                      | 78.23   | 82.76     | 56.11          | 91.80     | 70.95                  | 48.62   | 87.50   | 66.61   | 79.29   | 89.47   | 89.21     | 90.32   | 86.62                  | 81.49   | 97.60   | 92.44   | 88.73   | 72.36     | 45.38            | 56.34   | 90.68   | 51.89             | 90.32   | 83.95   | 74.23       | 79.33   | 66.44         | 92.32       | 92.44            | 90.32              | 74.23           | 83.95            | 70.95                | 86.62                | 71.80   | 56.34   | 77.17   | 92.60           |
| 2          | zeineb/LearningQ-t5-Answer-agnostic-QG | 78.02   | 82.78     | 92.83          | 72.03     | 92.46                  | 48.28   | 85.71   | 59.01   | 87.01   | 68.65   | 86.54     | 90.55   | 74.01                  | 66.64   | 76.07   | 86.68   | 90.81   | 58.65     | 93.69            | 92.46   | 87.88   | 74.49             | 74.01   | 86.68   | 85.00       | 77.95   | 55.48         | 89.12       | 90.55            | 81.49              | 51.48           | 74.49            | 85.00                | 70.42                | 89.60   | 54.93   | 72.49   | 85.53           |
| 3          | PSW/t5-base-tweetsumm-seed55           | 76.40   | 82.08     | 85.44          | 88.97     | 66.38                  | 48.25   | 77.74   | 80.36   | 81.02   | 55.00   | 76.20     | 90.25   | 85.10                  | 92.38   | 70.73   | 86.35   | 86.27   | 60.91     | 85.58            | 92.48   | 90.27   | 87.52             | 73.29   | 93.58   | 55.61       | 87.58   | 97.80         | 91.20       | 45.43            | 81.70              | 52.86           | 75.00            | 83.72                | 70.55                | 69.59   | 53.52   | 59.62   | 72.03           |
| 4          | PSW/t5-base-dialogsum-seed55           | 76.09   | 81.96     | 92.40          | 89.70     | 85.80                  | 45.72   | 77.89   | 76.79   | 80.54   | 55.00   | 85.75     | 90.35   | 44.58                  | 87.34   | 71.25   | 92.02   | 84.31   | 60.42     | 92.30            | 70.40   | 90.78   | 54.39             | 53.52   | 66.44   | 93.23       | 88.09   | 82.69         | 97.80       | 80.79            | 53.06              | 73.98           | 85.00            | 69.98                | 86.49                | 69.12   | 90.35   | 44.23   | 76.80           |
| 5          | ammarpl/t5-base-finetuned-xsum-a       | 75.57   | 82.47     | 83.20          | 92.70     | 66.54                  | 48.59   | 60.15   | 88.60   | 82.84   | 71.73   | 66.54     | 90.25   | 83.20                  | 92.44   | 88.84   | 54.85   | 87.01   | 77.63     | 71.19            | 92.44   | 90.70   | 56.34             | 76.53   | 93.23   | 55.20       | 78.10   | 52.19         | 82.55       | 39.04            | 82.55              | 52.19           | 77.17            | 81.51                | 71.19                | 85.48   | 56.34   | 98.40   | 93.23           |
| 6          | mlinda/conversation-summarizer         | 75.47   | 83.30     | 92.60          | 71.84     | 83.80                  | 49.56   | 87.50   | 52.00   | 81.98   | 61.39   | 85.90     | 90.55   | 45.02                  | 89.12   | 72.63   | 76.90   | 85.78   | 67.87     | 92.92            | 54.93   | 87.73   | 55.07             | 90.55   | 67.10   | 93.69       | 78.38   | 79.81         | 98.00       | 81.07            | 53.40              | 75.00           | 70.76            | 86.31                | 92.73                | 48.08   | 49.56   | 89.70   | 77.60           |
| 7          | ammarpl/t5-base-finetuned-eli5-a       | 75.39   | 82.16     | 45.49          | 91.60     | 48.44                  | 48.44   | 58.50   | 50.96   | 87.50   | 89.33   | 66.44     | 90.54   | 85.30                  | 92.42   | 98.20   | 86.93   | 90.47   | 70.66     | 86.93            | 92.42   | 90.47   | 75.81             | 75.81   | 90.54   | 56.34       | 87.50   | 70.06         | 84.77       | 45.49            | 72.70              | 54.34           | 76.79            | 84.77                | 70.06                | 72.37   | 56.34   | 84.69   | 84.62           |
| 8          | Muzzi/t5-base-finetuned-eli5           | 75.34   | 82.60     | 88.65          | 92.80     | 84.88                  | 49.28   | 69.91   | 64.42   | 81.69   | 71.77   | 92.80     | 49.28   | 70.43                  | 85.10   | 98.00   | 92.77   | 86.76   | 70.40     | 66.76            | 74.73   | 90.76   | 44.00             | 56.34   | 54.21   | 81.49       | 61.08   | 93.92         | 54.57       | 86.71            | 92.77              | 74.73           | 56.34            | 90.52                | 49.28                | 77.00   | 90.52   | 85.75   | 81.73           |
| 9          | Ghani-25/predy                         | 75.32   | 82.93     | 93.92          | 91.20     | 66.60                  | 50.03   | 60.83   | 66.77   | 81.11   | 89.30   | 53.67     | 90.32   | 82.80                  | 56.34   | 92.71   | 86.71   | 87.25   | 72.10     | 92.75            | 92.75   | 90.36   | 90.32             | 74.73   | 93.92   | 50.03       | 77.83   | 70.30         | 52.63       | 45.56            | 81.28              | 52.63           | 76.02            | 85.58                | 70.30                | 71.93   | 56.34   | 76.20   | 88.46           |
| 10         | helliun/conversational-qgen            | 75.29   | 81.89     | 83.00          | 97.60     | 48.19                  | 48.19   | 69.44   | 89.20   | 87.50   | 72.10   | 94.04     | 90.83   | 83.00                  | 70.12   | 88.09   | 86.58   | 76.27   | 76.10     | 74.87            | 92.60   | 82.14   | 86.58             | 77.98   | 90.83   | 92.60       | 58.81   | 52.46         | 45.70       | 45.70            | 81.21              | 52.46           | 74.87            | 83.60                | 70.12                | 85.74   | 54.93   | 91.80   | 55.25           |


<br>
<br>
Download full models ranking table: [csv](./results/t5-base_table.csv)

[Home](Home)