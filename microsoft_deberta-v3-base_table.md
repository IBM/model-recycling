---
layout: default
title: microsoft_deberta-v3-base
parent: Rankings
has_children: true
---
[comment]: # (This page contains a link to a table with the ranking and performance of all ranked microsoft_deberta-v3-base models. In addition, it contains a table with the baseline and the 10 best models. The original ranking was done by finetuning only the classification head of the model (linear probing) over the MNLI dataset.  The best models  by this ranking where ranked by the average accuracy after finetuning over the 36 datasets (except for the stsb dataset, where we used the Spearman correlation instead of accuracy).)

Ranking and performance of all 32 ranked microsoft_deberta-v3-base models ([full table](./results/microsoft_deberta-v3-base_table.csv)).  The top 24 models were fully tested.

Notes:
1. The baseline results can be found [here](microsoft_deberta-v3-base_pretrain_scores_table)
1. While the average improvement is small, many datasets show large gains

<br>


|            | model_name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | avg     | mnli_lp   | 20_newsgroup   | ag_news   | amazon_reviews_multi   | anli    | boolq   | cb      | cola    | copa    | dbpedia   | esnli   | financial_phrasebank   | imdb    | isear   | mnli    | mrpc    | multirc   | poem_sentiment   | qnli    | qqp     | rotten_tomatoes   | rte     | sst2    | sst_5bins   | stsb    | trec_coarse   | trec_fine   | tweet_ev_emoji   | tweet_ev_emotion   | tweet_ev_hate   | tweet_ev_irony   | tweet_ev_offensive   | tweet_ev_sentiment   | wic     | wnli    | wsc     | yahoo_answers   |
|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------|:----------|:---------------|:----------|:-----------------------|:--------|:--------|:--------|:--------|:--------|:----------|:--------|:-----------------------|:--------|:--------|:--------|:--------|:----------|:-----------------|:--------|:--------|:------------------|:--------|:--------|:------------|:--------|:--------------|:------------|:-----------------|:-------------------|:----------------|:-----------------|:---------------------|:---------------------|:--------|:--------|:--------|:----------------|
| *baseline* | *[microsoft/deberta-v3-base](microsoft/deberta-v3-base_pretrain_scores_table)*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | *79.04* | *nan*     | *86.41*        | *90.44*   | *66.86*                | *58.78* | *82.99* | *75.00* | *86.57* | *58.40* | *79.43*   | *91.93* | *84.48*                | *94.49* | *71.86* | *89.78* | *89.20* | *62.26*   | *86.73*          | *93.51* | *91.79* | *90.42*           | *82.35* | *95.06* | *56.98*     | *90.28* | *97.76*       | *91.02*     | *46.19*          | *83.95*            | *56.21*         | *79.82*          | *85.06*              | *71.80*              | *71.21* | *70.21* | *64.09* | *72.03*         |
| 1          | [sileod/deberta-v3-base_tasksource-420](model_gain_chart?avg=1.41&mnli_lp=nan&20_newsgroup=0.63&ag_news=0.46&amazon_reviews_multi=-0.40&anli=0.94&boolq=2.55&cb=10.71&cola=0.49&copa=10.60&dbpedia=0.10&esnli=-0.25&financial_phrasebank=1.31&imdb=-0.17&isear=0.63&mnli=0.42&mrpc=-0.23&multirc=1.73&poem_sentiment=0.77&qnli=0.12&qqp=-0.05&rotten_tomatoes=0.67&rte=2.13&sst2=0.01&sst_5bins=-0.02&stsb=1.39&trec_coarse=0.24&trec_fine=0.18&tweet_ev_emoji=0.62&tweet_ev_emotion=0.43&tweet_ev_hate=1.84&tweet_ev_irony=1.43&tweet_ev_offensive=0.17&tweet_ev_sentiment=0.08&wic=-1.78&wnli=3.03&wsc=9.95&yahoo_answers=0.17&model_name=sileod%2Fdeberta-v3-base_tasksource-420&base_name=microsoft%2Fdeberta-v3-base)                             | 80.45   | 89.82     | 87.04          | 90.90     | 66.46                  | 59.72   | 85.54   | 85.71   | 87.06   | 69.00   | 79.53     | 91.67   | 85.80                  | 94.32   | 72.49   | 90.21   | 88.97   | 63.99     | 87.50            | 93.63   | 91.74   | 91.09             | 84.48   | 95.07   | 56.97       | 91.67   | 98.00         | 91.20       | 46.81            | 84.38              | 58.05           | 81.25            | 85.23                | 71.88                | 69.44   | 73.24   | 74.04   | 72.20           |
| 2          | [MoritzLaurer/DeBERTa-v3-base-mnli](model_gain_chart?avg=0.97&mnli_lp=nan&20_newsgroup=-0.39&ag_news=0.19&amazon_reviews_multi=0.10&anli=1.31&boolq=0.81&cb=8.93&cola=0.01&copa=13.60&dbpedia=-0.23&esnli=-0.51&financial_phrasebank=0.61&imdb=-0.26&isear=-0.35&mnli=-0.34&mrpc=1.24&multirc=1.50&poem_sentiment=-0.19&qnli=0.30&qqp=0.13&rotten_tomatoes=-0.55&rte=3.57&sst2=0.35&sst_5bins=0.39&stsb=1.10&trec_coarse=-0.36&trec_fine=-0.02&tweet_ev_emoji=1.11&tweet_ev_emotion=-0.35&tweet_ev_hate=1.43&tweet_ev_irony=-2.65&tweet_ev_offensive=-1.69&tweet_ev_sentiment=-1.51&wic=0.57&wnli=-2.61&wsc=9.95&yahoo_answers=-0.33&model_name=MoritzLaurer%2FDeBERTa-v3-base-mnli&base_name=microsoft%2Fdeberta-v3-base)                             | 80.01   | 0.00      | 86.02          | 90.63     | 66.96                  | 60.09   | 83.79   | 83.93   | 86.58   | 72.00   | 79.20     | 91.42   | 85.10                  | 94.23   | 71.51   | 89.44   | 90.44   | 63.76     | 86.54            | 93.81   | 91.91   | 89.87             | 85.92   | 95.41   | 57.38       | 91.38   | 97.40         | 91.00       | 47.30            | 83.60              | 57.64           | 77.17            | 83.37                | 70.29                | 71.79   | 67.61   | 74.04   | 71.70           |
| 3          | [mariolinml/deberta-v3-base_MNLI_10_19_v0](model_gain_chart?avg=0.71&mnli_lp=nan&20_newsgroup=-0.57&ag_news=-0.21&amazon_reviews_multi=-0.12&anli=1.28&boolq=-1.15&cb=7.14&cola=-1.72&copa=10.60&dbpedia=0.00&esnli=-0.81&financial_phrasebank=2.42&imdb=-0.12&isear=-0.48&mnli=-0.06&mrpc=-0.97&multirc=2.12&poem_sentiment=1.73&qnli=0.25&qqp=0.08&rotten_tomatoes=-0.65&rte=3.21&sst2=0.12&sst_5bins=0.48&stsb=1.46&trec_coarse=-0.16&trec_fine=0.78&tweet_ev_emoji=-0.67&tweet_ev_emotion=0.29&tweet_ev_hate=-0.22&tweet_ev_irony=0.03&tweet_ev_offensive=-0.76&tweet_ev_sentiment=-0.54&wic=-1.15&wnli=4.44&wsc=-0.63&yahoo_answers=0.10&model_name=mariolinml%2Fdeberta-v3-base_MNLI_10_19_v0&base_name=microsoft%2Fdeberta-v3-base)             | 79.75   | 86.71     | 85.85          | 90.23     | 66.74                  | 60.06   | 81.83   | 82.14   | 84.85   | 69.00   | 79.43     | 91.11   | 86.90                  | 94.37   | 71.38   | 89.72   | 88.24   | 64.38     | 88.46            | 93.76   | 91.87   | 89.77             | 85.56   | 95.18   | 57.47       | 91.74   | 97.60         | 91.80       | 45.53            | 84.24              | 55.99           | 79.85            | 84.30                | 71.26                | 70.06   | 74.65   | 63.46   | 72.13           |
| 4          | [MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli](model_gain_chart?avg=0.65&mnli_lp=nan&20_newsgroup=-0.61&ag_news=-0.01&amazon_reviews_multi=0.46&anli=0.84&boolq=2.12&cb=16.07&cola=-0.76&copa=8.60&dbpedia=-0.40&esnli=-0.29&financial_phrasebank=-1.98&imdb=-0.47&isear=-0.22&mnli=-0.21&mrpc=0.50&multirc=1.91&poem_sentiment=1.73&qnli=0.07&qqp=-0.37&rotten_tomatoes=-0.74&rte=3.94&sst2=-0.45&sst_5bins=0.07&stsb=1.27&trec_coarse=-0.16&trec_fine=0.18&tweet_ev_emoji=-0.93&tweet_ev_emotion=-1.33&tweet_ev_hate=-1.67&tweet_ev_irony=-5.46&tweet_ev_offensive=-0.17&tweet_ev_sentiment=-0.11&wic=-0.21&wnli=-1.20&wsc=4.18&yahoo_answers=-0.70&model_name=MoritzLaurer%2FDeBERTa-v3-base-mnli-fever-anli&base_name=microsoft%2Fdeberta-v3-base) | 79.69   | 0.00      | 85.81          | 90.43     | 67.32                  | 59.62   | 85.11   | 91.07   | 85.81   | 67.00   | 79.03     | 91.63   | 82.50                  | 94.02   | 71.64   | 89.57   | 89.71   | 64.17     | 88.46            | 93.57   | 91.41   | 89.68             | 86.28   | 94.61   | 57.06       | 91.55   | 97.60         | 91.20       | 45.26            | 82.62              | 54.55           | 74.36            | 84.88                | 71.69                | 71.00   | 69.01   | 68.27   | 71.33           |
| 5          | [nc33/deberta_finetune](model_gain_chart?avg=0.47&mnli_lp=nan&20_newsgroup=-0.22&ag_news=-0.08&amazon_reviews_multi=0.62&anli=-0.22&boolq=1.36&cb=-1.79&cola=0.01&copa=9.60&dbpedia=0.23&esnli=-0.35&financial_phrasebank=4.11&imdb=-0.02&isear=0.37&mnli=-0.15&mrpc=0.99&multirc=1.27&poem_sentiment=0.77&qnli=0.05&qqp=-0.12&rotten_tomatoes=-0.18&rte=0.69&sst2=0.12&sst_5bins=1.39&stsb=0.13&trec_coarse=-0.56&trec_fine=-0.22&tweet_ev_emoji=0.93&tweet_ev_emotion=1.13&tweet_ev_hate=3.18&tweet_ev_irony=-0.74&tweet_ev_offensive=-1.34&tweet_ev_sentiment=-1.61&wic=-0.53&wnli=-2.61&wsc=0.34&yahoo_answers=0.30&model_name=nc33%2Fdeberta_finetune&base_name=microsoft%2Fdeberta-v3-base)                                                      | 79.51   | 75.33     | 86.19          | 90.37     | 67.48                  | 58.56   | 84.34   | 73.21   | 86.58   | 68.00   | 79.67     | 91.57   | 88.60                  | 94.47   | 72.23   | 89.64   | 90.20   | 63.53     | 87.50            | 93.56   | 91.67   | 90.24             | 83.03   | 95.18   | 58.37       | 90.41   | 97.20         | 90.80       | 47.12            | 85.08              | 59.39           | 79.08            | 83.72                | 70.20                | 70.69   | 67.61   | 64.42   | 72.33           |
| 6          | [nc33/finetune_rte_model](model_gain_chart?avg=0.34&mnli_lp=nan&20_newsgroup=0.56&ag_news=-0.04&amazon_reviews_multi=0.12&anli=0.59&boolq=1.57&cb=3.57&cola=0.01&copa=1.60&dbpedia=0.57&esnli=-0.80&financial_phrasebank=1.42&imdb=0.29&isear=0.63&mnli=-0.07&mrpc=-0.23&multirc=1.15&poem_sentiment=-1.15&qnli=0.10&qqp=0.34&rotten_tomatoes=0.20&rte=0.69&sst2=-0.11&sst_5bins=-1.01&stsb=1.43&trec_coarse=-0.16&trec_fine=0.18&tweet_ev_emoji=0.86&tweet_ev_emotion=-1.47&tweet_ev_hate=2.41&tweet_ev_irony=-1.51&tweet_ev_offensive=0.06&tweet_ev_sentiment=0.08&wic=0.73&wnli=0.21&wsc=-0.63&yahoo_answers=0.10&model_name=nc33%2Ffinetune_rte_model&base_name=microsoft%2Fdeberta-v3-base)                                                       | 79.38   | 78.60     | 86.98          | 90.40     | 66.98                  | 59.38   | 84.56   | 78.57   | 86.58   | 60.00   | 80.00     | 91.12   | 85.90                  | 94.78   | 72.49   | 89.71   | 88.97   | 63.41     | 85.58            | 93.61   | 92.13   | 90.62             | 83.03   | 94.95   | 55.97       | 91.70   | 97.60         | 91.20       | 47.05            | 82.48              | 58.62           | 78.32            | 85.12                | 71.88                | 71.94   | 70.42   | 63.46   | 72.13           |
| 7          | [SetFit/deberta-v3-base__sst2__all-train](model_gain_chart?avg=0.10&mnli_lp=nan&20_newsgroup=0.06&ag_news=0.36&amazon_reviews_multi=0.08&anli=0.63&boolq=1.45&cb=3.57&cola=0.39&copa=-1.40&dbpedia=0.57&esnli=-0.53&financial_phrasebank=1.52&imdb=-0.04&isear=-0.22&mnli=-0.19&mrpc=0.99&multirc=2.00&poem_sentiment=0.77&qnli=-0.19&qqp=0.21&rotten_tomatoes=-0.18&rte=-0.76&sst2=-0.34&sst_5bins=-0.60&stsb=-0.32&trec_coarse=0.24&trec_fine=-0.22&tweet_ev_emoji=0.82&tweet_ev_emotion=0.50&tweet_ev_hate=-3.92&tweet_ev_irony=-0.99&tweet_ev_offensive=-0.17&tweet_ev_sentiment=-0.96&wic=1.20&wnli=-2.61&wsc=2.26&yahoo_answers=-0.27&model_name=SetFit%2Fdeberta-v3-base__sst2__all-train&base_name=microsoft%2Fdeberta-v3-base)                | 79.14   | 0.00      | 86.47          | 90.80     | 66.94                  | 59.41   | 84.43   | 78.57   | 86.96   | 57.00   | 80.00     | 91.40   | 86.00                  | 94.45   | 71.64   | 89.60   | 90.20   | 64.25     | 87.50            | 93.32   | 91.99   | 90.24             | 81.59   | 94.72   | 56.38       | 89.96   | 98.00         | 90.80       | 47.01            | 84.45              | 52.29           | 78.83            | 84.88                | 70.84                | 72.41   | 67.61   | 66.35   | 71.77           |
| 8          | [bweb771/deberta_amazon_reviews_v1](model_gain_chart?avg=0.05&mnli_lp=nan&20_newsgroup=0.32&ag_news=-0.28&amazon_reviews_multi=-0.20&anli=-2.25&boolq=2.79&cb=3.57&cola=-0.28&copa=-3.40&dbpedia=-0.20&esnli=-0.63&financial_phrasebank=0.41&imdb=-0.20&isear=-0.42&mnli=-0.32&mrpc=0.26&multirc=0.28&poem_sentiment=3.65&qnli=-0.19&qqp=-0.12&rotten_tomatoes=-1.02&rte=1.41&sst2=-1.14&sst_5bins=0.62&stsb=-0.16&trec_coarse=0.04&trec_fine=-0.62&tweet_ev_emoji=-0.68&tweet_ev_emotion=0.64&tweet_ev_hate=-0.99&tweet_ev_irony=-0.48&tweet_ev_offensive=-0.64&tweet_ev_sentiment=0.06&wic=-0.05&wnli=-1.20&wsc=3.22&yahoo_answers=0.17&model_name=bweb771%2Fdeberta_amazon_reviews_v1&base_name=microsoft%2Fdeberta-v3-base)                        | 79.09   | 35.22     | 86.74          | 90.17     | 66.66                  | 56.53   | 85.78   | 78.57   | 86.29   | 55.00   | 79.23     | 91.30   | 84.90                  | 94.29   | 71.45   | 89.46   | 89.46   | 62.54     | 90.38            | 93.32   | 91.66   | 89.40             | 83.75   | 93.92   | 57.60       | 90.12   | 97.80         | 90.40       | 45.51            | 84.59              | 55.22           | 79.34            | 84.42                | 71.87                | 71.16   | 69.01   | 67.31   | 72.20           |
| 9          | [AI-Ahmed/deberta-v3-base-funetuned-cls-qqa](model_gain_chart?avg=0.04&mnli_lp=nan&20_newsgroup=-0.71&ag_news=0.16&amazon_reviews_multi=0.36&anli=-0.00&boolq=0.22&cb=5.36&cola=0.20&copa=5.60&dbpedia=0.07&esnli=-0.72&financial_phrasebank=-2.48&imdb=-0.22&isear=0.69&mnli=0.24&mrpc=-0.72&multirc=0.45&poem_sentiment=-3.08&qnli=0.30&qqp=0.03&rotten_tomatoes=0.01&rte=-1.48&sst2=0.01&sst_5bins=1.30&stsb=1.38&trec_coarse=-0.76&trec_fine=0.18&tweet_ev_emoji=1.13&tweet_ev_emotion=-0.21&tweet_ev_hate=4.73&tweet_ev_irony=-1.63&tweet_ev_offensive=0.29&tweet_ev_sentiment=0.40&wic=-1.47&wnli=-5.42&wsc=-2.55&yahoo_answers=-0.07&model_name=AI-Ahmed%2Fdeberta-v3-base-funetuned-cls-qqa&base_name=microsoft%2Fdeberta-v3-base)             | 79.08   | 75.66     | 85.70          | 90.60     | 67.22                  | 58.78   | 83.21   | 80.36   | 86.77   | 64.00   | 79.50     | 91.21   | 82.00                  | 94.27   | 72.56   | 90.02   | 88.48   | 62.71     | 83.65            | 93.81   | 91.82   | 90.43             | 80.87   | 95.07   | 58.28       | 91.65   | 97.00         | 91.20       | 47.32            | 83.74              | 60.94           | 78.19            | 85.35                | 72.20                | 69.75   | 64.79   | 61.54   | 71.97           |
| 10         | [deepset/deberta-v3-base-squad2](model_gain_chart?avg=0.04&mnli_lp=nan&20_newsgroup=-0.26&ag_news=-0.51&amazon_reviews_multi=0.10&anli=-1.25&boolq=2.46&cb=3.57&cola=-0.47&copa=13.60&dbpedia=0.10&esnli=-0.74&financial_phrasebank=-0.28&imdb=-0.34&isear=0.23&mnli=-0.22&mrpc=0.75&multirc=1.09&poem_sentiment=-3.08&qnli=0.36&qqp=0.04&rotten_tomatoes=-0.18&rte=1.05&sst2=-0.34&sst_5bins=1.93&stsb=1.16&trec_coarse=-0.56&trec_fine=0.38&tweet_ev_emoji=0.18&tweet_ev_emotion=-0.70&tweet_ev_hate=-0.49&tweet_ev_irony=-2.27&tweet_ev_offensive=-12.97&tweet_ev_sentiment=-0.87&wic=-0.37&wnli=1.62&wsc=-0.63&yahoo_answers=-0.60&model_name=deepset%2Fdeberta-v3-base-squad2&base_name=microsoft%2Fdeberta-v3-base)                              | 79.08   | 56.73     | 86.15          | 89.93     | 66.96                  | 57.53   | 85.44   | 78.57   | 86.10   | 72.00   | 79.53     | 91.18   | 84.20                  | 94.15   | 72.10   | 89.56   | 89.95   | 63.35     | 83.65            | 93.87   | 91.83   | 90.24             | 83.39   | 94.72   | 58.91       | 91.44   | 97.20         | 91.40       | 46.37            | 83.25              | 55.72           | 77.55            | 72.09                | 70.93                | 70.85   | 71.83   | 63.46   | 71.43           |


<br>
<br>
Download full models ranking table: [csv](./results/microsoft_deberta-v3-base_table.csv)

[Home](.)