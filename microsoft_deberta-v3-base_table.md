---
layout: default
title: microsoft_deberta-v3-base
parent: Rankings
has_children: true
---
[comment]: # (This page contains a link to a table with the ranking and performance of all ranked microsoft_deberta-v3-base models. In addition, it contains a table with the baseline and the 10 best models. The original ranking was done by finetuning only the classification head of the model (linear probing) over the MNLI dataset.  The best models  by this ranking where ranked by the average accuracy after finetuning over the 36 datasets (except for the stsb dataset, where we used the Spearman correlation instead of accuracy).)

Ranking and performance of all 14 ranked microsoft_deberta-v3-base models ([full table](./results/microsoft_deberta-v3-base_table.csv)).  The top 14 models were fully tested.

Notes:
1. The baseline results can be found [here](microsoft_deberta-v3-base_pretrain_scores_table)
1. While the average improvement is small, many datasets show large gains

<br>


|            | model_name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | avg     | mnli_lp   | 20_newsgroup   | ag_news   | amazon_reviews_multi   | anli    | boolq   | cb      | cola    | copa    | dbpedia   | esnli   | financial_phrasebank   | imdb    | isear   | mnli    | mrpc    | multirc   | poem_sentiment   | qnli    | qqp     | rotten_tomatoes   | rte     | sst2    | sst_5bins   | stsb    | trec_coarse   | trec_fine   | tweet_ev_emoji   | tweet_ev_emotion   | tweet_ev_hate   | tweet_ev_irony   | tweet_ev_offensive   | tweet_ev_sentiment   | wic     | wnli    | wsc     | yahoo_answers   |
|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------|:----------|:---------------|:----------|:-----------------------|:--------|:--------|:--------|:--------|:--------|:----------|:--------|:-----------------------|:--------|:--------|:--------|:--------|:----------|:-----------------|:--------|:--------|:------------------|:--------|:--------|:------------|:--------|:--------------|:------------|:-----------------|:-------------------|:----------------|:-----------------|:---------------------|:---------------------|:--------|:--------|:--------|:----------------|
| *baseline* | *[microsoft/deberta-v3-base](microsoft/deberta-v3-base_pretrain_scores_table)*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | *79.04* | *nan*     | *86.41*        | *90.44*   | *66.86*                | *58.78* | *82.99* | *75.00* | *86.57* | *58.40* | *79.43*   | *91.93* | *84.48*                | *94.49* | *71.86* | *89.78* | *89.20* | *62.26*   | *86.73*          | *93.51* | *91.79* | *90.42*           | *82.35* | *95.06* | *56.98*     | *90.28* | *97.76*       | *91.02*     | *46.19*          | *83.95*            | *56.21*         | *79.82*          | *85.06*              | *71.80*              | *71.21* | *70.21* | *64.09* | *72.03*         |
| 1          | [MoritzLaurer/DeBERTa-v3-base-mnli](model_gain_chart?avg=0.97&mnli_lp=nan&20_newsgroup=-0.39&ag_news=0.19&amazon_reviews_multi=0.10&anli=1.31&boolq=0.81&cb=8.93&cola=0.01&copa=13.60&dbpedia=-0.23&esnli=-0.51&financial_phrasebank=0.61&imdb=-0.26&isear=-0.35&mnli=-0.34&mrpc=1.24&multirc=1.50&poem_sentiment=-0.19&qnli=0.30&qqp=0.13&rotten_tomatoes=-0.55&rte=3.57&sst2=0.35&sst_5bins=0.39&stsb=1.10&trec_coarse=-0.36&trec_fine=-0.02&tweet_ev_emoji=1.11&tweet_ev_emotion=-0.35&tweet_ev_hate=1.43&tweet_ev_irony=-2.65&tweet_ev_offensive=-1.69&tweet_ev_sentiment=-1.51&wic=0.57&wnli=-2.61&wsc=9.95&yahoo_answers=-0.33&model_name=MoritzLaurer%2FDeBERTa-v3-base-mnli&base_name=microsoft%2Fdeberta-v3-base)                             | 80.01   | 89.77     | 86.02          | 90.63     | 66.96                  | 60.09   | 83.79   | 83.93   | 86.58   | 72.00   | 79.20     | 91.42   | 85.10                  | 94.23   | 71.51   | 89.44   | 90.44   | 63.76     | 86.54            | 93.81   | 91.91   | 89.87             | 85.92   | 95.41   | 57.38       | 91.38   | 97.40         | 91.00       | 47.30            | 83.60              | 57.64           | 77.17            | 83.37                | 70.29                | 71.79   | 67.61   | 74.04   | 71.70           |
| 2          | [MoritzLaurer/DeBERTa-v3-base-mnli](model_gain_chart?avg=0.97&mnli_lp=nan&20_newsgroup=-0.39&ag_news=0.19&amazon_reviews_multi=0.10&anli=1.31&boolq=0.81&cb=8.93&cola=0.01&copa=13.60&dbpedia=-0.23&esnli=-0.51&financial_phrasebank=0.61&imdb=-0.26&isear=-0.35&mnli=-0.34&mrpc=1.24&multirc=1.50&poem_sentiment=-0.19&qnli=0.30&qqp=0.13&rotten_tomatoes=-0.55&rte=3.57&sst2=0.35&sst_5bins=0.39&stsb=1.10&trec_coarse=-0.36&trec_fine=-0.02&tweet_ev_emoji=1.11&tweet_ev_emotion=-0.35&tweet_ev_hate=1.43&tweet_ev_irony=-2.65&tweet_ev_offensive=-1.69&tweet_ev_sentiment=-1.51&wic=0.57&wnli=-2.61&wsc=9.95&yahoo_answers=-0.33&model_name=MoritzLaurer%2FDeBERTa-v3-base-mnli&base_name=microsoft%2Fdeberta-v3-base)                             | 80.01   | 0.00      | 86.02          | 90.63     | 66.96                  | 60.09   | 83.79   | 83.93   | 86.58   | 72.00   | 79.20     | 91.42   | 85.10                  | 94.23   | 71.51   | 89.44   | 90.44   | 63.76     | 86.54            | 93.81   | 91.91   | 89.87             | 85.92   | 95.41   | 57.38       | 91.38   | 97.40         | 91.00       | 47.30            | 83.60              | 57.64           | 77.17            | 83.37                | 70.29                | 71.79   | 67.61   | 74.04   | 71.70           |
| 3          | [MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli](model_gain_chart?avg=0.65&mnli_lp=nan&20_newsgroup=-0.61&ag_news=-0.01&amazon_reviews_multi=0.46&anli=0.84&boolq=2.12&cb=16.07&cola=-0.76&copa=8.60&dbpedia=-0.40&esnli=-0.29&financial_phrasebank=-1.98&imdb=-0.47&isear=-0.22&mnli=-0.21&mrpc=0.50&multirc=1.91&poem_sentiment=1.73&qnli=0.07&qqp=-0.37&rotten_tomatoes=-0.74&rte=3.94&sst2=-0.45&sst_5bins=0.07&stsb=1.27&trec_coarse=-0.16&trec_fine=0.18&tweet_ev_emoji=-0.93&tweet_ev_emotion=-1.33&tweet_ev_hate=-1.67&tweet_ev_irony=-5.46&tweet_ev_offensive=-0.17&tweet_ev_sentiment=-0.11&wic=-0.21&wnli=-1.20&wsc=4.18&yahoo_answers=-0.70&model_name=MoritzLaurer%2FDeBERTa-v3-base-mnli-fever-anli&base_name=microsoft%2Fdeberta-v3-base) | 79.69   | 89.93     | 85.81          | 90.43     | 67.32                  | 59.62   | 85.11   | 91.07   | 85.81   | 67.00   | 79.03     | 91.63   | 82.50                  | 94.02   | 71.64   | 89.57   | 89.71   | 64.17     | 88.46            | 93.57   | 91.41   | 89.68             | 86.28   | 94.61   | 57.06       | 91.55   | 97.60         | 91.20       | 45.26            | 82.62              | 54.55           | 74.36            | 84.88                | 71.69                | 71.00   | 69.01   | 68.27   | 71.33           |
| 4          | [MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli](model_gain_chart?avg=0.65&mnli_lp=nan&20_newsgroup=-0.61&ag_news=-0.01&amazon_reviews_multi=0.46&anli=0.84&boolq=2.12&cb=16.07&cola=-0.76&copa=8.60&dbpedia=-0.40&esnli=-0.29&financial_phrasebank=-1.98&imdb=-0.47&isear=-0.22&mnli=-0.21&mrpc=0.50&multirc=1.91&poem_sentiment=1.73&qnli=0.07&qqp=-0.37&rotten_tomatoes=-0.74&rte=3.94&sst2=-0.45&sst_5bins=0.07&stsb=1.27&trec_coarse=-0.16&trec_fine=0.18&tweet_ev_emoji=-0.93&tweet_ev_emotion=-1.33&tweet_ev_hate=-1.67&tweet_ev_irony=-5.46&tweet_ev_offensive=-0.17&tweet_ev_sentiment=-0.11&wic=-0.21&wnli=-1.20&wsc=4.18&yahoo_answers=-0.70&model_name=MoritzLaurer%2FDeBERTa-v3-base-mnli-fever-anli&base_name=microsoft%2Fdeberta-v3-base) | 79.69   | 0.00      | 85.81          | 90.43     | 67.32                  | 59.62   | 85.11   | 91.07   | 85.81   | 67.00   | 79.03     | 91.63   | 82.50                  | 94.02   | 71.64   | 89.57   | 89.71   | 64.17     | 88.46            | 93.57   | 91.41   | 89.68             | 86.28   | 94.61   | 57.06       | 91.55   | 97.60         | 91.20       | 45.26            | 82.62              | 54.55           | 74.36            | 84.88                | 71.69                | 71.00   | 69.01   | 68.27   | 71.33           |
| 5          | [SetFit/deberta-v3-base__sst2__all-train](model_gain_chart?avg=0.10&mnli_lp=nan&20_newsgroup=0.06&ag_news=0.36&amazon_reviews_multi=0.08&anli=0.63&boolq=1.45&cb=3.57&cola=0.39&copa=-1.40&dbpedia=0.57&esnli=-0.53&financial_phrasebank=1.52&imdb=-0.04&isear=-0.22&mnli=-0.19&mrpc=0.99&multirc=2.00&poem_sentiment=0.77&qnli=-0.19&qqp=0.21&rotten_tomatoes=-0.18&rte=-0.76&sst2=-0.34&sst_5bins=-0.60&stsb=-0.32&trec_coarse=0.24&trec_fine=-0.22&tweet_ev_emoji=0.82&tweet_ev_emotion=0.50&tweet_ev_hate=-3.92&tweet_ev_irony=-0.99&tweet_ev_offensive=-0.17&tweet_ev_sentiment=-0.96&wic=1.20&wnli=-2.61&wsc=2.26&yahoo_answers=-0.27&model_name=SetFit%2Fdeberta-v3-base__sst2__all-train&base_name=microsoft%2Fdeberta-v3-base)                | 79.14   | 35.22     | 86.47          | 90.80     | 66.94                  | 59.41   | 84.43   | 78.57   | 86.96   | 57.00   | 80.00     | 91.40   | 86.00                  | 94.45   | 71.64   | 89.60   | 90.20   | 64.25     | 87.50            | 93.32   | 91.99   | 90.24             | 81.59   | 94.72   | 56.38       | 89.96   | 98.00         | 90.80       | 47.01            | 84.45              | 52.29           | 78.83            | 84.88                | 70.84                | 72.41   | 67.61   | 66.35   | 71.77           |
| 6          | [SetFit/deberta-v3-base__sst2__all-train](model_gain_chart?avg=0.10&mnli_lp=nan&20_newsgroup=0.06&ag_news=0.36&amazon_reviews_multi=0.08&anli=0.63&boolq=1.45&cb=3.57&cola=0.39&copa=-1.40&dbpedia=0.57&esnli=-0.53&financial_phrasebank=1.52&imdb=-0.04&isear=-0.22&mnli=-0.19&mrpc=0.99&multirc=2.00&poem_sentiment=0.77&qnli=-0.19&qqp=0.21&rotten_tomatoes=-0.18&rte=-0.76&sst2=-0.34&sst_5bins=-0.60&stsb=-0.32&trec_coarse=0.24&trec_fine=-0.22&tweet_ev_emoji=0.82&tweet_ev_emotion=0.50&tweet_ev_hate=-3.92&tweet_ev_irony=-0.99&tweet_ev_offensive=-0.17&tweet_ev_sentiment=-0.96&wic=1.20&wnli=-2.61&wsc=2.26&yahoo_answers=-0.27&model_name=SetFit%2Fdeberta-v3-base__sst2__all-train&base_name=microsoft%2Fdeberta-v3-base)                | 79.14   | 0.00      | 86.47          | 90.80     | 66.94                  | 59.41   | 84.43   | 78.57   | 86.96   | 57.00   | 80.00     | 91.40   | 86.00                  | 94.45   | 71.64   | 89.60   | 90.20   | 64.25     | 87.50            | 93.32   | 91.99   | 90.24             | 81.59   | 94.72   | 56.38       | 89.96   | 98.00         | 90.80       | 47.01            | 84.45              | 52.29           | 78.83            | 84.88                | 70.84                | 72.41   | 67.61   | 66.35   | 71.77           |
| 7          | [ItuThesis2022MlviNikw/deberta-v3-base](model_gain_chart?avg=-1.40&mnli_lp=nan&20_newsgroup=-0.47&ag_news=0.16&amazon_reviews_multi=0.08&anli=-0.31&boolq=-0.75&cb=0.00&cola=0.78&copa=-3.40&dbpedia=-0.33&esnli=-0.76&financial_phrasebank=2.11&imdb=-0.04&isear=1.08&mnli=0.03&mrpc=-1.46&multirc=1.65&poem_sentiment=-0.19&qnli=-0.35&qqp=-0.39&rotten_tomatoes=0.29&rte=-3.29&sst2=0.58&sst_5bins=-1.24&stsb=0.38&trec_coarse=-0.36&trec_fine=-0.42&tweet_ev_emoji=0.68&tweet_ev_emotion=-0.70&tweet_ev_hate=1.09&tweet_ev_irony=-9.67&tweet_ev_offensive=0.52&tweet_ev_sentiment=-0.89&wic=0.26&wnli=-30.77&wsc=-4.47&yahoo_answers=0.33&model_name=ItuThesis2022MlviNikw%2Fdeberta-v3-base&base_name=microsoft%2Fdeberta-v3-base)                | 77.64   | 61.58     | 85.94          | 90.60     | 66.94                  | 58.47   | 82.23   | 75.00   | 87.34   | 55.00   | 79.10     | 91.16   | 86.60                  | 94.45   | 72.95   | 89.81   | 87.75   | 63.90     | 86.54            | 93.15   | 91.40   | 90.71             | 79.06   | 95.64   | 55.75       | 90.66   | 97.40         | 90.60       | 46.87            | 83.25              | 57.31           | 70.15            | 85.58                | 70.91                | 71.47   | 39.44   | 59.62   | 72.37           |
| 8          | [ItuThesis2022MlviNikw/deberta-v3-base](model_gain_chart?avg=-1.40&mnli_lp=nan&20_newsgroup=-0.47&ag_news=0.16&amazon_reviews_multi=0.08&anli=-0.31&boolq=-0.75&cb=0.00&cola=0.78&copa=-3.40&dbpedia=-0.33&esnli=-0.76&financial_phrasebank=2.11&imdb=-0.04&isear=1.08&mnli=0.03&mrpc=-1.46&multirc=1.65&poem_sentiment=-0.19&qnli=-0.35&qqp=-0.39&rotten_tomatoes=0.29&rte=-3.29&sst2=0.58&sst_5bins=-1.24&stsb=0.38&trec_coarse=-0.36&trec_fine=-0.42&tweet_ev_emoji=0.68&tweet_ev_emotion=-0.70&tweet_ev_hate=1.09&tweet_ev_irony=-9.67&tweet_ev_offensive=0.52&tweet_ev_sentiment=-0.89&wic=0.26&wnli=-30.77&wsc=-4.47&yahoo_answers=0.33&model_name=ItuThesis2022MlviNikw%2Fdeberta-v3-base&base_name=microsoft%2Fdeberta-v3-base)                | 77.64   | 0.00      | 85.94          | 90.60     | 66.94                  | 58.47   | 82.23   | 75.00   | 87.34   | 55.00   | 79.10     | 91.16   | 86.60                  | 94.45   | 72.95   | 89.81   | 87.75   | 63.90     | 86.54            | 93.15   | 91.40   | 90.71             | 79.06   | 95.64   | 55.75       | 90.66   | 97.40         | 90.60       | 46.87            | 83.25              | 57.31           | 70.15            | 85.58                | 70.91                | 71.47   | 39.44   | 59.62   | 72.37           |
| 9          | [Emanuel/twitter-emotion-deberta-v3-base](model_gain_chart?avg=-1.40&mnli_lp=nan&20_newsgroup=0.35&ag_news=0.29&amazon_reviews_multi=-0.00&anli=-0.00&boolq=-2.10&cb=-5.36&cola=-0.85&copa=-2.40&dbpedia=0.27&esnli=-0.77&financial_phrasebank=1.02&imdb=-0.63&isear=-0.48&mnli=0.16&mrpc=-1.21&multirc=-0.11&poem_sentiment=2.69&qnli=0.32&qqp=0.27&rotten_tomatoes=-1.02&rte=-5.45&sst2=0.01&sst_5bins=-0.02&stsb=-1.43&trec_coarse=0.04&trec_fine=-0.02&tweet_ev_emoji=1.16&tweet_ev_emotion=-0.28&tweet_ev_hate=-0.02&tweet_ev_irony=-2.78&tweet_ev_offensive=-1.45&tweet_ev_sentiment=0.17&wic=-0.37&wnli=-18.10&wsc=-11.20&yahoo_answers=-1.14&model_name=Emanuel%2Ftwitter-emotion-deberta-v3-base&base_name=microsoft%2Fdeberta-v3-base)       | 77.64   | 46.42     | 86.76          | 90.73     | 66.86                  | 58.78   | 80.89   | 69.64   | 85.71   | 56.00   | 79.70     | 91.15   | 85.50                  | 93.86   | 71.38   | 89.94   | 87.99   | 62.15     | 89.42            | 93.83   | 92.05   | 89.40             | 76.90   | 95.07   | 56.97       | 88.85   | 97.80         | 91.00       | 47.35            | 83.67              | 56.20           | 77.04            | 83.60                | 71.97                | 70.85   | 52.11   | 52.88   | 70.90           |
| 10         | [Emanuel/twitter-emotion-deberta-v3-base](model_gain_chart?avg=-1.40&mnli_lp=nan&20_newsgroup=0.35&ag_news=0.29&amazon_reviews_multi=-0.00&anli=-0.00&boolq=-2.10&cb=-5.36&cola=-0.85&copa=-2.40&dbpedia=0.27&esnli=-0.77&financial_phrasebank=1.02&imdb=-0.63&isear=-0.48&mnli=0.16&mrpc=-1.21&multirc=-0.11&poem_sentiment=2.69&qnli=0.32&qqp=0.27&rotten_tomatoes=-1.02&rte=-5.45&sst2=0.01&sst_5bins=-0.02&stsb=-1.43&trec_coarse=0.04&trec_fine=-0.02&tweet_ev_emoji=1.16&tweet_ev_emotion=-0.28&tweet_ev_hate=-0.02&tweet_ev_irony=-2.78&tweet_ev_offensive=-1.45&tweet_ev_sentiment=0.17&wic=-0.37&wnli=-18.10&wsc=-11.20&yahoo_answers=-1.14&model_name=Emanuel%2Ftwitter-emotion-deberta-v3-base&base_name=microsoft%2Fdeberta-v3-base)       | 77.64   | 0.00      | 86.76          | 90.73     | 66.86                  | 58.78   | 80.89   | 69.64   | 85.71   | 56.00   | 79.70     | 91.15   | 85.50                  | 93.86   | 71.38   | 89.94   | 87.99   | 62.15     | 89.42            | 93.83   | 92.05   | 89.40             | 76.90   | 95.07   | 56.97       | 88.85   | 97.80         | 91.00       | 47.35            | 83.67              | 56.20           | 77.04            | 83.60                | 71.97                | 70.85   | 52.11   | 52.88   | 70.90           |


<br>
<br>
Download full models ranking table: [csv](./results/microsoft_deberta-v3-base_table.csv)

[Home](Home)