---
layout: default
title: google_t5-v1_1-base
parent: Rankings
has_children: true
---
[comment]: # (This page contains a link to a table with the ranking and performance of all ranked google_t5-v1_1-base models. In addition, it contains a table with the baseline and the 10 best models. The original ranking was done by finetuning only the classification head of the model (linear probing) over the MNLI dataset.  The best models  by this ranking where ranked by the average accuracy after finetuning over the 36 datasets (except for the stsb dataset, where we used the Spearman correlation instead of accuracy).)

Ranking and performance of all 13 ranked google_t5-v1_1-base models ([full table](./results/google_t5-v1_1-base_table.csv)).  7 models are fully tested.

Notes:
1. The baseline results can be found [here](google_t5-v1_1-base_pretrain_scores_table)
1. While the average improvement is small, many datasets show large gains
<br>


|            | model_name                                            | avg     | mnli_lp   | 20_newsgroup   | ag_news   | amazon_reviews_multi   | anli    | boolq   | cb      | cola    | copa    | dbpedia   | esnli   | financial_phrasebank   | imdb    | isear   | mnli    | mrpc    | multirc   | poem_sentiment   | qnli    | qqp     | rotten_tomatoes   | rte     | sst2    | sst_5bins   | stsb    | trec_coarse   | trec_fine   | tweet_ev_emoji   | tweet_ev_emotion   | tweet_ev_hate   | tweet_ev_irony   | tweet_ev_offensive   | tweet_ev_sentiment   | wic     | wnli    | wsc     | yahoo_answers   |
|:-----------|:------------------------------------------------------|:--------|:----------|:---------------|:----------|:-----------------------|:--------|:--------|:--------|:--------|:--------|:----------|:--------|:-----------------------|:--------|:--------|:--------|:--------|:----------|:-----------------|:--------|:--------|:------------------|:--------|:--------|:------------|:--------|:--------------|:------------|:-----------------|:-------------------|:----------------|:-----------------|:---------------------|:---------------------|:--------|:--------|:--------|:----------------|
| *baseline* | *google/t5-v1_1-base*                                 | *68.82* | *nan*     | *82.88*        | *88.18*   | *66.91*                | *38.06* | *65.57* | *55.45* | *70.18* | *40.50* | *70.77*   | *85.58* | *66.74*                | *92.99* | *71.06* | *75.51* | *72.83* | *56.14*   | *68.08*          | *89.37* | *83.60* | *86.05*           | *60.58* | *93.72* | *51.84*     | *68.79* | *93.25*       | *82.07*     | *33.46*          | *75.61*            | *51.52*         | *67.62*          | *82.61*              | *69.88*              | *55.84* | *46.90* | *48.32* | *69.26*         |
| 1          | anshoomehra/t5-v1-base-s2-auto-qgen                   | 74.27   | 80.67     | 88.37          | 76.37     | 82.09                  | 48.09   | 54.00   | 60.93   | 75.74   | 63.46   | 83.65     | 48.09   | 69.97                  | 45.48   | 91.60   | 91.56   | 90.19   | 88.70     | 66.26            | 69.68   | 87.56   | 78.47             | 56.34   | 74.74   | 53.20       | 70.95   | 92.66         | 54.48       | 86.63            | 91.56              | 56.34           | 90.01            | 69.97                | 86.63                | 70.73   | 90.01   | 71.70   | 97.60           |
| 2          | anshoomehra/t5-v1-base-s-q-auto-qgen                  | 72.49   | 78.71     | 97.00          | 70.53     | 44.65                  | 44.50   | 69.76   | 55.00   | 76.41   | 60.48   | 90.80     | 89.74   | 76.85                  | 52.49   | 70.63   | 56.34   | 80.88   | 65.99     | 86.40            | 89.74   | 89.68   | 92.32             | 44.50   | 81.30   | 66.12       | 84.68   | 92.36         | 83.65       | 52.05            | 70.54              | 84.30           | 70.69            | 89.68                | 62.45                | 36.54   | 56.34   | 88.70   | 85.41           |
| 3          | anshoomehra/t5-v1-base-s-q-multi-task-qgen            | 72.22   | 77.46     | 69.61          | 65.80     | 71.81                  | 45.16   | 76.07   | 84.48   | 89.66   | 91.76   | 83.49     | 89.36   | 83.49                  | 65.80   | 53.06   | 45.16   | 55.00   | 87.15     | 92.43            | 90.28   | 60.52   | 77.50             | 68.59   | 53.06   | 76.92       | 65.36   | 45.16         | 90.28       | 69.61            | 84.58              | 90.28           | 68.59            | 52.11                | 89.36                | 52.08   | 52.11   | 92.43   | 71.81           |
| 4          | ClueAI/PromptCLUE                                     | 69.31   | 70.39     | 83.46          | 88.77     | 55.00                  | 43.44   | 69.02   | 60.71   | 68.55   | 55.00   | 75.97     | 88.74   | 43.83                  | 81.33   | 61.93   | 88.74   | 79.17   | 61.14     | 88.12            | 43.44   | 84.46   | 44.84             | 57.76   | 65.66   | 89.11       | 82.37   | 72.12         | 94.80       | 50.71            | 81.74              | 66.40           | 83.89            | 86.18                | 53.52                | 64.11   | 53.52   | 59.62   | 67.87           |
| 5          | ClueAI/PromptCLUE                                     | 69.31   | 0.00      | 83.46          | 88.77     | 55.00                  | 43.44   | 69.02   | 60.71   | 68.55   | 55.00   | 75.97     | 88.74   | 43.83                  | 81.33   | 61.93   | 88.74   | 79.17   | 61.14     | 88.12            | 43.44   | 84.46   | 44.84             | 57.76   | 65.66   | 89.11       | 82.37   | 72.12         | 94.80       | 50.71            | 81.74              | 66.40           | 83.89            | 86.18                | 53.52                | 64.11   | 53.52   | 59.62   | 67.87           |
| 6          | ClueAI/PromptCLUE-base                                | 67.60   | 70.68     | 88.51          | 67.33     | 66.00                  | 43.62   | 55.00   | 58.66   | 68.55   | 64.11   | 86.80     | 43.62   | 83.89                  | 71.85   | 75.97   | 59.21   | 80.15   | 59.62     | 43.83            | 53.52   | 82.37   | 51.11             | 88.22   | 81.74   | 68.49       | 69.02   | 54.90         | 42.76       | 86.18            | 59.21              | 88.22           | 43.62            | 83.89                | 86.18                | 89.00   | 43.62   | 61.41   | 83.46           |
| 7          | ClueAI/PromptCLUE-base                                | 67.60   | 0.00      | 88.51          | 67.33     | 66.00                  | 43.62   | 55.00   | 58.66   | 68.55   | 64.11   | 86.80     | 43.62   | 83.89                  | 71.85   | 75.97   | 59.21   | 80.15   | 59.62     | 43.83            | 53.52   | 82.37   | 51.11             | 88.22   | 81.74   | 68.49       | 69.02   | 54.90         | 42.76       | 86.18            | 59.21              | 88.22           | 43.62            | 83.89                | 86.18                | 89.00   | 43.62   | 61.41   | 83.46           |
| 8          | anshoomehra/t5-v1_1-base-squadAndQnliAutoQgenAutoQgen | nan     | 0.00      | nan            | nan       | nan                    | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan                    | nan     | nan     | nan     | nan     | nan       | nan              | nan     | nan     | nan               | nan     | nan     | nan         | nan     | nan           | nan         | nan              | nan                | nan             | nan              | nan                  | nan                  | nan     | nan     | nan     | nan             |
| 9          | anshoomehra/temp                                      | nan     | 0.00      | nan            | nan       | nan                    | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan                    | nan     | nan     | nan     | nan     | nan       | nan              | nan     | nan     | nan               | nan     | nan     | nan         | nan     | nan           | nan         | nan              | nan                | nan             | nan              | nan                  | nan                  | nan     | nan     | nan     | nan             |
| 10         | binxu/mengzi-t5-base-finetuned-punctuation            | nan     | 0.00      | nan            | nan       | nan                    | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan                    | nan     | nan     | nan     | nan     | nan       | nan              | nan     | nan     | nan               | nan     | nan     | nan         | nan     | nan           | nan         | nan              | nan                | nan             | nan              | nan                  | nan                  | nan     | nan     | nan     | nan             |


<br>
<br>
Download full models ranking table: [csv](./results/google_t5-v1_1-base_table.csv)

[Home](Home)