---
layout: default
title: Home
nav_order: 0
image: "Twitter_card.png"
description: "Model-recycling - the best model per architecture. Comparing finetuned models from HF, as base models for future finetune on texts. "

---
# Welcome to model-recycling page

Hardly anyone trains from scratch anymore, we all finetune over a pretrained model. 

[Research](https://arxiv.org/abs/2211.00107) slowly reaches consensus that some finetuned models are better base models than the pretrained models
themselves.

This site presents a dynamic view of the best models to choose for a given model size and architecture.
We follow the findings and methodology from our Our [paper](https://arxiv.org/abs/2211.00107):
We download finetuned models found in HuggingFace per architecture and efficiently rank them over a representative task.
 We then evaluate the top ranked models by finetuning over a large set of 36 target tasks, and report the average
 performance of each base model.

Tested so far: $$SUCCESSFULLY_TESTED$$ (and counting)
## Best models per architectures
<br>

$$BEST_PER_MODEL$$

<br>
<br>

To learn more see our [FAQ](faq) or read the paper. See detailed evaluation results on each architecture [here](Rankings).
If you have any feedback or question please [contact us](contact_us).

<span style="font-size:0.8em;">This work was performed in IBM Research by Leshem Choshen, Elad Venezian, Shachar Don-Yehiya, Noam Slonim and Yoav Katz.</span>
