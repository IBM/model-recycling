<!DOCTYPE html>
<html>
<head>
</head>
<body>
  <h1>FAQ</h1>
  <h2>Why should I use a different base model than the vanilla pretrained model?</h2>
<p> It improves results significantly in most cases, and in average. The best base RoBERTa-besa models improve results in 75% of the tasks we evaluated, with a median gain of 2.5 accuracy points. So if you had have to choose one base model, it would be best to use these top ranked models.</p>
<h2>Can I get worse results from training over the top ranked base model when compared to the vanilla model?</h2>
  <p>Yes. For example, in RoBERTa-base, about 1 in 4 tasks perform slightly better on the pretrained model. Furthermore, difference in seed randomization can yield variance in results. The best approach is to assess multiple models and evaluate on dev data. You can see example HuggingFace code that streamline this process. <p>
<h2>When shouldnâ€™t I use one of the recommended base models?</h2>
<p> You should always review the base model license and fact sheet to ensure they meet your requirement for the particular use case.</p>
<h2>Which architectures are supported?</h2>
<p>In the initial version, Roberta-base models are tracked. Other architectures will be added soon. Want us to add a specific model? Please contact us and say so. If you have recommended training parameters, it is even better, send them too. </p>
<h2>How frequently do you update the leaderboard?</h2>
<p>We will update the results monthly.</p>
<h2>How do you assess the models?</h2>
<p>We train a linear probing classification head for the MNLI on each candidate model.  We take each of the top 5 ranking models, and we fine-tune them on the 36 classification tasks (Consisting of sentiment, NLI, Twitter, topic classification and other general classification tasks).   We compare to the baseline of the vanilla model which is also trained and assessed on 5 seeds.   We use the following hyperparameters:</p>
<h2>I have another question.</h2>
  <p>Please <a href="contact_us.html">contact us</a></p>
</body>
</html>